---
title: "The Process"
author: "Madison Hobbs"
date: "6/19/2017"
output: pdf_document
bibliography: rpoSsummer2017.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(DESeq2)
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(cluster) #PAM
library(mclust) #adjusted rand index
```

#Table of Contents
1. Background
2. RNA-Seq, Briefly
3. Start With the Raw Read Counts
4. Quality Control
5. Normalize All Counts
6. Find Your Regulon Using Differential Expression Analysis
7. Grouping Methods

#1. Background
The goal of the research of Dr. Stoebel, Dr. Hardin, and their many contributors is to assess how the level of gene expression of each gene in the genome of E. coli is influenced by RpoS, a protein which is known to regulate the stress response of E. coli (you can find a lot more information on the RpoS wikipedia page).  See the prior research with three RpoS levels in @Wong2017. 

#2. RNA-Seq, Breifly
RNA Sequencing (RNA-seq) is a way to measure gene expression in an organism. The RNA is extracted and RNA reads (sequences of A, G, C, and U) are aligned to the organism's genome. The higher the number of reads mapped to a certain gene, the more we can say that gene is expressed. The gene annotations come in different features, CDS (coding sequence), ncRNA, rRNA and tRNA. IGR is another feature which represents intergenic regions, spaces between two genes.

The features rRNA and tRNA are removed as much as possible from the RNA that gets sequenced, but you will likely still find reads of those two features (though hopefully in small proportions). If you're working in a data set where the feature type is not specified, and each gene appears only once, each gene's read count is likely the sum of sens and antisens CDS reads. To keep it simple, in this demonstration we'll work with that type of situation. 

#3. Start With the Raw Read Counts
```{r}
allCounts <- read.csv("DMS2670_LB_NC_000913.tsv", header = T, sep = "\t")
#collect the geneids
allCounts$GeneidBackup = allCounts$Geneid
# note that the geneids here are really long and contain a lot of information. We are going to parse those out, and first...
# make a separate column for gene feature (CDS, AS_CDS, IGR, etc). 
allCounts <- allCounts %>% separate(GeneidBackup, c("feature", "rest"), sep="[:]")
allCounts <- allCounts %>% select(Geneid, feature,                                          
         "0.00_A" = A0, # relabel the columns to reflect RpoS level, not arabinose
         "0.00_B" = B0,
         "0.00_C" = C0,
         "0.35_A" = A10..5,
         "0.35_B" = B10..5,
         "0.35_C" = C10..5,
        "11.59_A" = A5x10.5,
         "11.59_B" = B5x10.5,
        "11.59_C" = C5x10.5,
        "20.40_A" = A10..4,
        "20.40_B" = B10..4,
        "20.40_C" = C10..4,
        "48.37_A" = A10..3,
        "48.37_B" = B10..3,
        "48.37_C" = C10..3,
       "100.00_A" = A2537,
       "100.00_B" = B2537,
       "100.00_C" = C2537,
       "129.96_A" = A2x10..3,
       "129.96_B" = B2x10..3,
       "129.96_C" = C2x10..3,
       "190.38_A" = A5x10..3,
       "190.38_B" = B5x10..3,
       "190.38_C" = C5x10..3)

allCounts %>% group_by(feature) %>% summarise(number_of_genes = n())
```
Above, we see the breakdown by feature in our data. The AS before the first five rows simply means "anti-sens" meaning that the read was read backwards, so to speak, but it still belongs to the feature after the "_". For every read in our data, there an antisens read.
CDS means coding sequence; these will have bnumbers (bnum). IGR stands for intergenic region; these are regions between genes. ncRNA stands for non-coding RNA. rRNA is ribosomal RNA and tRNA is transfer RNA; these are both removed as much as possible lest the dominate the read counts, but some slip in as we can see.  

```{r}
# Now, we must extract the genenames from each Geneid. However, each feature has a slightly different pattern, and we will need to do the features separately. 

# IGR's (this includes AS_IGRSs): 
# IGR stands for intergenic region which means a region between coding sequences or different types of RNA. Therefore, we'll have a start.bnum and end.bnum as well as a start.genename and end.genename. Note that not all genes will have a bnum (only CDS/AS_CDS do)
bnum = "b[0-9]{4}" # what do bnumbers look like?
genename = ",[a-z]{3}[A-Z,]." # what does a genename look like? this is regexp lingo
rna.name = ",rna[0-9].." # what does an RNA name look like?
igr <- allCounts %>% filter(feature %in% c("IGR", "AS_IGR"))
igr$GeneidBackup = igr$Geneid # store the Geneid
igr <- igr %>% separate(GeneidBackup, c("Geneid1", "Geneid2"), sep = "[/]") # separate the first part of the Geneid which talks about the IGR's start gene (Geneid1) and the last part of the IGR Geneid which talks about that IGR's end gene (Geneid2). 
igr$feature1 <- separate(igr, Geneid1, c("feature1", "rest"), sep = "[,]")$feature1
igr$feature1 <- separate(igr, feature1, c("rest", "feature1"), sep = "[()]")$feature1 #start feature
igr$feature2 <- separate(igr, Geneid2, c("feature2", "rest"), sep = "[,]")$feature2
igr$start.gene <- case_when( #start gene name: many possibilities!
    igr$feature1 == "CDS" ~ str_extract(igr$Geneid1, genename), #if the start feature was CDS, then the name is going to be genename style, so we extract a genename-type thing from Geneid1
    TRUE ~ str_extract(igr$Geneid1, rna.name)) #otherwise, it's going to have an RNA-style name, so we extract the rna.name from Geneid1
igr$end.gene <- case_when( #end gene neame: similar to above!
    igr$feature2 == "CDS" ~ str_extract(igr$Geneid2, genename), # if the end feature was CDS, then we're looking for a genename-type bit from Geneid2
    TRUE ~ str_extract(igr$Geneid2, rna.name)) #otherwise, it must be an RNA-style label of some sort. 
igr$start.bnum <- case_when(
    igr$feature1 == "CDS" ~ str_extract(igr$Geneid1, bnum), #bnums only exist for CDS, so we check if the feature is CDS before extracting a bnum from Geneid1
    TRUE ~ "none") # if not CDS, then no bnum exists so we can put "none"
igr$end.bnum <- case_when(
    igr$feature2 == "CDS" ~ str_extract(igr$Geneid2, bnum), #same thing as above but for end bnum
    TRUE ~ "none")
# now get rid of all those pesky commas that got into our start.gene labels. I could have not included the punctuation in my regex pattern, but then str_extract() might have gotten confused with a less specific pattern
igr <- igr %>% separate(start.gene, into = c("comma", "start.gene"), sep = "[,]") %>% select(-comma) %>% separate(end.gene, into = c("comma", "end.gene"), sep = "[,]") %>% select(-comma)
allCounts <- full_join(igr, allCounts) #add this new information to allCounts!
```
```{r, echo=F, warning = F, message = F}
# CDS
# have bnum and genename columns
# left join to allCounts
genename = ":[a-z]{3}.." #new genename pattern
#bnum pattern stays the same
cds <- allCounts %>% filter(feature %in% c("AS_CDS", "CDS")) 
cds$genename <- str_extract(cds$Geneid, genename) #extract those genenames!
cds$bnum <- str_extract(cds$Geneid, bnum) # extract them bnums!
#get rid of the pesky colon that was part of the pattern
cds <- cds %>% separate(genename, into = c("colon", "genename"), sep = ":") %>%
  select(-colon)
allCounts <- full_join(allCounts, cds) #add the new info to allCounts
```
```{r, echo=F, warning = F, message = F}
#ncRNA
#ncRNA doesn't have bnums, but id's which we'll put in the genename column
rna.name = ":rna[0-9].." #new rna.name pattern
rna <- allCounts %>% filter(feature %in% c("ncRNA", "AS_ncRNA"))
rna$genename <- str_extract(rna$Geneid, rna.name) #record those rna.names
rna <- rna %>% separate(genename, into = c("colon", "genename"), sep = ":") %>%
  select(-colon) #get rid of colon
allCounts <- full_join(allCounts, rna) #update allCounts with ncRNA names!

#rRNA
rRNA <- allCounts %>% filter(feature %in% c("rRNA", "AS_rRNA"))
rRNA$genename <- str_extract(rRNA$Geneid, rna.name) #same rna.name pattern exists as above, so extract those rna.names! And store the result as genename - I know. This is just for convenience's sake so we have a common column to refer to when we want the short hand name for a gene. 
rRNA <- rRNA %>% separate(genename, into = c("colon", "genename"), sep = ":") %>%
  select(-colon) #get rid of that colon
allCounts <- full_join(allCounts, rRNA) #update allCounts

#tRNA -- analogous to rRNA above. 
tRNA <- allCounts %>% filter(feature %in% c("tRNA", "AS_tRNA"))
tRNA$genename <- str_extract(tRNA$Geneid, rna.name)
tRNA <- tRNA %>% separate(genename, into = c("colon", "genename"), sep = ":") %>%
  select(-colon)
allCounts <- full_join(tRNA, allCounts)

# remove the NA rows we just created by full_joining while adding the ncRNA, rRNA, tRNA genenames
allCounts <- filter(allCounts, feature %in% c("IGR", "AS_IGR") | genename != "NA")

# make tidy data
allCounts.tidy <- allCounts %>%
  gather(cond.samps, rawCount, -Geneid, -feature, -genename, -bnum, -Geneid1, -Geneid2, -feature1, -feature2, -start.gene, -end.gene, -start.bnum, - end.bnum)
```
What we get back from RNA Sequencing is a data table where each row represents a gene and each column represents an experimental condition and its sample (for instance, column A1 could be condition A, sample 1). Hopefully, your data has more than one sample per condition. These are called "replicates," and they help us know what is really going on at each condition (the more the better!). Each observation in this large data table is the number of reads for that row's gene at that column's sample. 

By "raw," I mean unnormalized. See below for a description of normalization. 

#4. Quality Control
Think about quality control issues and in what ways you might want to clean your data. For us, we decided to remove the 100% condition because the E. coli used in all other conditions had an rssB deletion, and thus would not be comparable. We also decided to remove the columns (samples) for which the median read count across the entire sample was below 10. We also removed rows (genes) for which the maximum read count across all samples was under 5. The logic was that with read counts so low, these samples and genes would not allow us to reliably distinguish between signal and noise. See more about this and other concerns with the data used in this demonstration in Questions_Regarding_2017_Data_Simplified. 

#5. Normalize All Counts
```{r}
# keep only genes whose maximum raw count is greater than 5 and exclude the samples `0.00_B`, `0.35_B`, and `20.40_A` whose Q3's are below 10. We also exclude the samples with condition 100% since these are a different strain of E. Coli
allCounts.tidy <- allCounts.tidy %>% group_by(Geneid) %>%
  filter(max(rawCount) > 5) %>% filter(cond.samps %in% c("0.00_B", "0.35_B", "20.40_A", "100.00_A", "100.00_B", "100.00_C") == FALSE)
#remomve the conditions we don't want

# create wide, messy version of this which DESeq2 needs. Filter out the unwanted columns but not the unwanted rows; we're going to need these for normalization
allCounts <- allCounts %>% select(Geneid, feature, `0.00_A`, `0.00_C`,
         `0.35_A`, `0.35_C`, 
         `11.59_A`, `11.59_B`, `11.59_C`, 
         `20.40_B`, `20.40_C`, 
         `48.37_A`, `48.37_B`, `48.37_C`, 
         `129.96_A`, `129.96_B`, `129.96_C`, 
         `190.38_A`, `190.38_B`, `190.38_C`) %>%
  filter(Geneid %in% allCounts.tidy$Geneid) #keep only the rows that satisfied the condition above

countsTable.allCond <- allCounts %>% select(`0.00_A`, `0.00_C`, 
 `0.35_A`, `0.35_C`,  `11.59_A`, `11.59_B`, `11.59_C`, `20.40_B`, `20.40_C`, 
`48.37_A`, `48.37_B`, `48.37_C`, `129.96_A`, `129.96_B`, `129.96_C`,  
`190.38_A`, `190.38_B`, `190.38_C`)

# Define the conditions.
conditions.allCond <- as.factor(c("0.00", "0.00", "0.35", "0.35", "11.59", "11.59", "11.59", "20.40", "20.40", "48.37", "48.37", "48.37", "129.96", "129.96", "129.96", "190.38", "190.38", "190.38"))
coldata.allCond <- as.data.frame(row.names = colnames(countsTable.allCond), conditions.allCond)
# create DESeqDataSet
dds.allCond <- DESeqDataSetFromMatrix(countData = countsTable.allCond, colData = coldata.allCond, design = ~conditions.allCond)
# DESeq(dds.allCond) produces the normalized counts, among other things
dds.allCond <- DESeq(dds.allCond)
# the part we really care about: extract the normalized counts
normalizedCounts <- as.data.frame(counts(dds.allCond, normalized = TRUE)) %>% 
  mutate(Geneid = allCounts$Geneid) # re-attach the gene names

#make tidy normalized count data
normCounts.tidy <- normalizedCounts %>% gather(cond.samps, normCount, -Geneid)

# add the normalized counts to our big allCounts.tidy data table
allCounts.tidy <- inner_join(allCounts.tidy, normCounts.tidy)

allCounts.tidy <- allCounts.tidy %>%
  separate(cond.samps, c("RpoS%", "replicate"), sep = "[_]") %>%
  mutate(`RpoS%` = as.numeric(`RpoS%`)) %>%
  group_by(Geneid, `RpoS%`) %>%
  mutate(levelMean = mean(normCount)) %>% #create level mean which is the mean normCount between the replicates at each sample
  ungroup() %>%
  group_by(Geneid) %>%
  mutate(scaler = ifelse(levelMean[1] < levelMean[18], levelMean[18], levelMean[1]),
         #scale by the maximum between levelMean at the lowest RpoS condition (levelMean[1]) and levelMean at the highest RpoS condition (levelMean[18])
         normCountScaled = normCount/scaler, #scale each normCount by scaler
         meanScaled = levelMean/scaler) %>% #scale all the level means by the scaler
  ungroup() 
```
We must normalize the counts because each sample is sequenced at a different depth in RNA Seq. The sequencing depth of each sample refers to the total raw read count (column sum) of that sample. If we simply used the raw read counts from the data table given us by RNASeq, we wouldn't be comparing apples to apples because of that difference in sequencing depth. For instance, if the gene smpB has 3 reads at condition 20% RpoS sample A and has 87 reads at condition 60% RpoS sample A, it might appear that smpB is really differentially expressed across conditions 20% and 60%. But perhaps condition 60% sample A has a much higher sequencing depth than condition 20% sample A. Normalization accounts for problems like this. There are many different techniques, they are really fun to think about, and you should read about them and the theory behind them in Dr. Hardin's, Dr. Stoebel's, and Ciaran Evan's paper (@Ciaran). You can also read more about the process of DESeq in the appendix of Ciaran Evan's thesis (4). 

We use DESeq's normalization technique which is a standard method for this type of analysis. DESeq normalizes in the following way: Start with one sample. For each gene, take the raw count of that gene at that sample over the geometric mean of the raw read count for that gene across all samples. Do this for all genes, and you get a sample-specific vector of numbers. Take the median of that vector, and you have the "size factor" for your sample. Size factors essentially tell us how deeply the sample is sequenced, and the ratio of the size factors of two different samples is an approximation of the ratio of their sequencing depths.

#6. Find Your Regulon Using Differential Expression Analysis
```{r, message=FALSE, warning=FALSE}
# Make sure you have loaded the DESeq2 package
# compare 0 to 190.38
countsTable.0.190.38 <- allCounts %>%
  select(`0.00_A`, `0.00_C`, `190.38_A`, `190.38_B`, `190.38_C`)
rownames(countsTable.0.190.38) <- allCounts$Geneid
# define conditions
condition.0.190.38 <- as.factor(c("0.00", "0.00", "190.38", "190.38", "190.38"))
# we only want the relevant column name from countsTable for each condition (we don't want Geneid or genename)
rownames <- as.data.frame(colnames(countsTable.0.190.38))
coldata.0.190.38 <- data.frame(row.names = colnames(countsTable.0.190.38), condition.0.190.38)
# create the DESeqDataSet. cds stands for "count data set"
dds.0.190.38 <- DESeqDataSetFromMatrix(countData = countsTable.0.190.38, colData = coldata.0.190.38, design = ~condition.0.190.38)
# estimate size factors, dispersions, etc, and testing differential expression
dds.0.190.38 <- DESeq(dds.0.190.38)
#estimating dispersion by treating samples as replicates
```
We are interested only in the genes that RpoS "regulates," or the genes whose gene expression RpoS affects. Although RpoS, as a primary regulator of E. coli's general stress response, regulates many genes in the E. coli genome, about two thirds of the genes will be unaffected by RpoS. 

As is consistent with previous studies, we define the "regulon" (genes regulated by RpoS) as genes which exhibit differential expression between 0% RpoS (knock-out, where the gene rpoS coding for the protein RpoS is literally knocked out of the specimen's genome) and 100% RpoS (wild-type (WT), or the level of RpoS in the natural, wild E. coli). In the cases where we do not have a 100% RpoS level, as is true in this data, we differentially express between the lowest and highest conditions (in our case, 0% vs 190.38%). Differential expression analysis is a technique we use to assess how differnently each gene is  being expressed across the experimental conditions. We use the R package DESeq2 from Bioconductor (@Love2014, see footnote 1).  

DESeq first estimates "size factors" for each sample. As mentioned previously, the size factors for, say, sample 0.00_A are calculated in the following way. For imaginary gene uboD, for instance, uboD's raw read count for 0.00_A is divided by the geometric mean of uboD's raw read counts across 0.00_A, 0.00_B, 100.00_A, and 100.00_B We obtain these values for every gene, take the median of those values, and the result is our size factor for the sample 0.00_A. Didn't we already normalize, you may ask? When we remove the other samples to keep only 0% and 190.38%, the size factors are going to change. We want our decision about regulation between 0% and 190.38% RpoS based only on the counts for those two conditions, not affected by different sequencing depths of other samples. It is those normalized counts we produce which shall be compared to teset for significant difference across the conditions. 

Next, DESeq estimates "dispersions," which is a measure of how much the variance deviates from the mean. DESEq uses a negative binomial probability model to perform hypothesis testing (Wald test) on differential expression between conditions, and this model has two parameters: mean and dispersion. The negative binomial probability model can be thought of as a generalized Poisson probability model, because in Poisson, dispersion is 1 so mean = variance (@Maechler2017). 
Finally, DESeq2 fits a negative binomial probability model to perform hypotheis testing, using a Wald test, to find a p-value for each gene being differentially expressed across the two conditions. Thesse p-values are FDR-adjusted (Benjamini-Hochberg) for multiple comparisons to control the rate of false discoveries (see footnote 2). 

Using the results() function from DESeq2 (see below), we extract the p-values and FDR-adjusted p-values. The fuction summary() is very useful in giving you the number of genes differentially expressed between the two conditions. LFC stands for the log of the fold change between two conditions (where fold change is normalized counts at condition 2 divided by normalized counts at condition 1; for instance, if the noramlized counts for gene uboD at 190.38% RpoS was twice that at 0% RpoS, we would have a fold change of 2, and the log fold change would be log(2) = 0.69). Positive LFC's show that a gene's expression increased from condition 1 to 2 (upregulation), and negative LFC's show that a gene's expression decreased from condition 1 to 2 (downregulation). Summary() summarizes the number of genes which, at the specified confidence level, show significant differential expression across the two condtions, outlining how many of these are upregulated, downregulated, outliers, or low counts.
```{r}
#get results
results.0.190.38 <- results(dds.0.190.38, alpha = 0.05)
summary(results.0.190.38)
```
The results summary shows the number and percentage of genes up- and down-regulated from 0% to 190.38%; these will form our "regulon."
```{r}
# extract the information about the regulated genes and store the information with the table of allCounts and the allCounts.tidy dataframess
Geneids <- rownames(countsTable.0.190.38)
regulated.genes <- as.data.frame(results.0.190.38) %>% mutate(Geneid = Geneids, regulation = ifelse(log2FoldChange > 0, "positive", "negative"))
regulated.genes <- filter(regulated.genes, padj < 0.05)

#modify allCounts to include regulation information
allCounts <- left_join(allCounts, regulated.genes, by = "Geneid")

#modify allCounts.tidy.filterd to include regulation information
allCounts.tidy <- left_join(allCounts.tidy, regulated.genes, by = "Geneid")
```

#7. Grouping Methods
##Profile Assignment
One approach is to create one's one gene expression profiles based on questions one has about the gene expression shapes. For example, the underlying question we have been trying to answer is how sensitive genes are to increased levels of RpoS; is gene expression affected even at low levels of RpoS, or does it take more RpoS to make an impact on gene expression? To these ends, we have designed six ad-hoc profiles (sensitive positive, sensitive negative, insensitive positive, insensitive negative, linear positive, linear negative) to categorize gene expression, shown below. However, if you are interested in a different question, you can design any number of your own profiles.

Note: All of our plots of gene expression represent gene expression as a proportion of the highest mean normalized count between 0% replicates and 190.38% replicates. The y-axis will represents this proportion of gene expression (normCountScaled) and the x-axis represents the percentage of RpoS inputed (RpoS%).
```{r}
#extrapolated from profiles as seen in Madison's reproduction and reanalysis of Garrett's code (Re-Rewritten_RNAseq_Analysis_2017)
profile.linear.pos <- c(0,0, 0.0035,0.0035, 0.1159,0.1159,0.1159, 0.2040,0.2040, 0.4837,0.4837,0.4837, 1,1,1, 1,1,1)
profile.linear.neg <- c(1,1, 0.9965,0.9965, 0.8841,0.8841,0.8841, 
0.7960, 0.7960, 0.5163,0.5163,0.5163, 0,0,0, 0,0,0)
profile.sens.pos <- c(0,0, 0,0, 0.4,0.4,0.4, 0.71,0.71, 0.93,0.93,0.93, 1,1,1, 1,1,1)
profile.sens.neg <- c(1,1, 1,1, 0.6,0.6,0.6, 0.29,0.29, 0.07,0.07,0.07, 0,0,0, 0,0,0)
profile.insens.pos <- c(0,0, 0,0, 0.04,0.04,0.04, 0.08,0.08, 0.37,0.37,0.37, 1,1,1, 1,1,1)
profile.insens.neg <- c(1,1, 1,1, 0.96,0.96,0.96, 0.92,0.92, 0.63,0.63,0.63, 0,0,0, 0,0,0)

# data frame of all six profiles side by side
profile.6 <- data.frame(sens.pos = profile.sens.pos, sens.neg = profile.sens.neg, insens.pos = profile.insens.pos, insens.neg = profile.insens.neg, linear.pos = profile.linear.pos, linear.neg = profile.linear.neg)

#make profiles data table tidy 
profiles.tidy <- profile.6 %>% mutate(`RpoS%` = c(0,0, 0.0035,0.0035, 0.1159,0.1159,0.1159, 0.2040,0.2040, 0.4837,0.4837,0.4837, 1.2996,1.2996,1.2996, 1.9038,1.9038,1.9038)) %>% gather(group, gene.expression, -`RpoS%`)
# plot the six profiles
ggplot(profiles.tidy, aes(x = `RpoS%`, y = gene.expression)) + geom_point(size = 2) + geom_line(size = 2) + facet_grid(. ~ group) + ylab("Gene Expression") + ggtitle("6 Profiles") +  theme(plot.title = element_text(size = 10, face = "bold"))
```
Next, we take each gene and correlate its expression (normCount) with each of the profiles above. The gene is then sorted with the profile for which it was most correlated. 
```{r}
# gather normalized counts which we will be correlating with the profiles
normCounts.forCorr <- allCounts.tidy %>%
  filter(regulation != "NA") %>%
  select(Geneid, `RpoS%`, replicate, normCount) %>%
  unite(cond.samps, `RpoS%`, replicate) %>%
  reshape2::dcast(cond.samps ~ Geneid, value.var = "normCount") %>%
  select(-1)

# correlate normalized counts with each of the six profiles, and sort each gene with the profile with which it has a maximum correlation (with linear as the tiebreaker).
profile.normCounts.forCorr <- as.data.frame(t(cor(profile.6, normCounts.forCorr)))
rownames <- rownames(profile.normCounts.forCorr)
rowMax <- rowMax(as.matrix(profile.normCounts.forCorr))
profile.normCounts.forCorr <- profile.normCounts.forCorr %>%
  mutate(Geneid = rownames,
         maxCorr = rowMax,
         group = ifelse(maxCorr == linear.pos, "linear.pos", 
                        ifelse(maxCorr == linear.neg, "linear.neg",
                               ifelse(maxCorr == insens.pos, "insens.pos",
                                      ifelse(maxCorr == insens.neg, "insens.neg",
                                             ifelse(maxCorr == sens.pos, "sens.pos", "sens.neg"))))))

# add info to allCounts.tidy
allCounts.tidy <- left_join(allCounts.tidy, profile.normCounts.forCorr, by = "Geneid")
#how many in each category?
allCounts.tidy %>% filter(regulation != "NA") %>%
  group_by(group)  %>% summarise(n()/18)
allCounts.tidy %>% filter(regulation != "NA") %>%
  group_by(regulation, group)  %>% summarise(n()/18)

#plot!
ggplot(data = filter(allCounts.tidy, regulation != "NA"), aes(x = `RpoS%`, y = normCountScaled)) + geom_point(aes(col = Geneid)) + geom_line(data = filter(allCounts.tidy, regulation != "NA"), aes(y = meanScaled, col = Geneid), linetype = "dashed") + theme(legend.position = "none") + facet_grid(. ~ group, labeller = label_both) + coord_cartesian(ylim = c(0,2)) + ggtitle("Profile Assignment") + ylab("Gene Expression") + theme(plot.title = element_text(size = 10, face = "bold"))
```
Here are good things to look at: correlation strength and differentiability. 
```{r}
summary(allCounts.tidy$maxCorr) # the correlations used to sort each gene
summary(filter(allCounts.tidy, maxCorr == sens.neg)$maxCorr) #look at sensitive negative correlations individually
#check differentiability between sensitive negative and linear negative
summary(filter(allCounts.tidy, maxCorr == sens.neg)$maxCorr - filter(allCounts.tidy, maxCorr == sens.neg)$linear.neg)
```

##Intermediate Differential Expression Testing
If our question is whether a gene is so sensitive that increasing RpoS from 0% to 11.59% has an impact on gene expression, we can use differential expression analysis to answer this question. This alternative is the only that offers p-values and weeds out genes with inter-replicate variability in a way that the previous and following methods do not. 

```{r}
# compare 0 to 0.1159
countsTable.0.1159 <- allCounts %>%
  select(`0.00_A`, `0.00_C`,`11.59_A`, `11.59_B`,  `11.59_C`)
# define conditions
condition.0.1159 <- as.factor(c("0.00", "0.00", "11.59",  "11.59", "11.59"))
coldata.0.1159 <- data.frame(row.names = colnames(countsTable.0.1159), condition.0.1159)
rownames(countsTable.0.1159) = allCounts$Geneid
# create the DESeqDataSet. cds stands for "count data set"
cds.0.1159 <- DESeqDataSetFromMatrix(countData = countsTable.0.1159, colData = coldata.0.1159, design = ~condition.0.1159)
# estimate size factors, dispersions, etc, and testing differential expression
cds.0.1159 <- DESeq(cds.0.1159)
#estimating dispersion by treating samples as replicates
#get results
results.0.1159 <- results(cds.0.1159, alpha = 0.05/2) #bonferroni adjustment because we did the comparison 0% vs 190.38% AND 0% vs 11.59%
summary(results.0.1159)

# extract the information about the genes DE 0% vs 11.59% and store the information with the table of allCounts and the allCounts.tidy dataframess
Geneids <- rownames(countsTable.0.1159)
superSensitive.genes <- as.data.frame(results.0.1159) %>% mutate(Geneid = Geneids, direction.0.1159 = ifelse(log2FoldChange > 0, "positive", "negative"))
superSensitive.genes <- filter(superSensitive.genes, padj < 0.05/2, 
                               Geneid %in% regulated.genes$Geneid) %>% #keep only significantly DE genes and ones that are in regulon
  select(Geneid, direction.0.1159)
#modify allCounts to include regulation information
allCounts <- left_join(allCounts, superSensitive.genes, by = "Geneid")
#modify allCounts.tidy.filterd to include regulation information
allCounts.tidy <- left_join(allCounts.tidy, superSensitive.genes, by = "Geneid")

DE.0.1159.tidy <- filter(allCounts.tidy, direction.0.1159 != "NA")
#plot
ggplot(data = DE.0.1159.tidy, aes(x = `RpoS%`, y = normCountScaled, col=Geneid)) + geom_point() + geom_line(data = DE.0.1159.tidy, aes(y = meanScaled), linetype = "dashed") + coord_cartesian(ylim = c(0,2)) + theme(legend.position = "none") + facet_grid(. ~ regulation, labeller = "label_both") + ggtitle("Super Sensitive Gene Expression: Genes Differentially Expressed 0% vs 11.59%") + ylab("Gene Expression") + theme(plot.title = element_text(size = 10, face = "bold"))
# I noted that the genes positive between 0% and 11.59% are the same genes positive between 0% and 190.38%, and same for negative, so I only facet by regulation (0% vs 190.38% direction of regulation)
```

Note that we have to adjust for multiple comparisons with this approach (I use Bonferroni above). One could continue this method to produce all possible combinations and patterns to categorise the genes based on thier differential expression across different RpoS levels.

##PAM Clustering (Partitioning Around Medoids)
Another approach is to let an algorithm like PAM (@Kaufman1990) find the most typical gene expression patterns for us, and group genes based on these. This answers a slightly different question. Instead of asking "how many genes look most like this particular shape I'm interested in" as above, now we ask in an unsupervised way "what are the most common shapes in my data?"

PAM works by finding the k observations (genes) which make the tightest and most differentiable clusters. These k observations are called "medoids," and all genes in the data are assigned to the closest of the k medoids. There are a variety ways we can measure "closest," but we choose to use correlation because we are more interested in differences between the shapes of gene expression than in the euclidean distance, for instance, between plotted points. 
```{r}
# make wide format that the PAM algorithm likes
countsToCluster <- select(allCounts.tidy, Geneid, `RpoS%`, replicate, normCount) %>%
  unite(cond.samp, `RpoS%`, replicate, sep = "_") %>% spread(cond.samp, normCount)
# only keep the genes that are in the regulon
countsToCluster <- countsToCluster %>%  filter(Geneid %in% regulated.genes$Geneid) 
# save the Geneids and make these the names of the rows, keeping only the columns with counts.
Geneids <- countsToCluster$Geneid
countsToCluster <- countsToCluster %>% select(-Geneid)
rownames(countsToCluster) <- Geneids
#generate dissmilarity matrix based on spearman correlation
dissimilarity.matrix <- 1 - cor(t(countsToCluster), method = "spearman")

#one iteration of PAM with k = 6 (6 clusters)
pam.6 <- pam(dissimilarity.matrix, 6)
pam.6.medoids <- data.frame(rownames(pam.6$medoids),  pam.6$silinfo$clus.avg.widths)
# contains the names of the genes which are the medoids for each cluster (around which each cluster is centered) and the silhouette width of each cluster: the larger the silhouette width, the more distinct that cluster is from the others. 
pam.6.clusters <- data.frame(pam.6$clustering)
Geneids <- rownames(pam.6.clusters) # extract Geneids
pam.6.clusters <- pam.6.clusters %>% mutate(Geneid = Geneids)
allCounts.tidy <- left_join(allCounts.tidy, pam.6.clusters) #store information with allCounts.tidy
# just the regulated genes to plot
regulated.tidy <- filter(allCounts.tidy, Geneid %in% regulated.genes$Geneid)

regulated.tidy %>% group_by(regulation, pam.6.clustering) %>% summarise(numGenes = n()/18) %>% arrange(pam.6.clustering)
```
After letting PAM cluster the genes into 6 groups, we see the break-down above of the number of genes in each cluster and how many were found positively and negatively regulated by differential expression analysis on 0% versus 190.38%. The clustering agrees almost perfectly with the separation pertaining to up- and down-regulation found by differential expression. 
```{r}
# find medoids normCounts to plot
medoids.toplot <- filter(regulated.tidy, Geneid %in% pam.6.medoids$rownames.pam.6.medoids.)

#plot
ggplot(data = regulated.tidy, aes(x = `RpoS%`, y = normCountScaled, col=Geneid)) + geom_point() + geom_line(data = regulated.tidy, aes(y = meanScaled), linetype = "dashed") + coord_cartesian(ylim = c(0,2)) + theme(legend.position = "none") + facet_grid(. ~ pam.6.clustering) + ggtitle(paste("Gene Expression Clustering, PAM k=6; Medoids Overlaid in black; avg silhouette width = ", round(pam.6$silinfo$avg.width, 3))) + ylab("Gene Expression") + geom_line(data = medoids.toplot, aes(y = meanScaled), col = "black", size = 2) + theme(plot.title = element_text(size = 10, face = "bold"))
#Compare this plot to the plot with k=6 in Grouping_Genes_by_Patterns_of_Gene_Expression - the medoids and clusters are the same.
```

In the plot above, we see the six clusters PAM produced plotted in terms of their Rpos level and gene expression. The medoid gene with which they were clustered plotted us over each cluster in black. 

Average silhouette width, measured between -1 and 1, is a metric of how well each gene is classified into each cluster. The closer this value is to 1, the more differentiable the clusters which indicates stronger evidence that there are distinct k distinct clusters in the data. Low and negative average silhouette width suggests you have chosen too many or too few clusters. The average silhouette width for k = 6 is 0.320, which is not too low, but also not as high as we might like. 

A next step would be to let PAM cluster on a range of values of k, examining silhouette width as well as the plots themselves. Even if we found higher silhouette width with another k value, this is just telling us the number of clusters that gives the most prominent difference between clusters. We may still want to think of our data in terms six clusters because we are interested in the six most typical gene expression shapes exhibited. 

We can also compare grouping methods using the Adjusted Rand Index, or ARI which measures how similarly two clustering methods classified objects. An ARI of -1 would suggest that the two methods are in perfect disagreement, and an ARI of 1 would suggest that the methods are in perfect agreement. An ARI of 0 would suggest that the two methods are no better matched than they would be by random chance (based on the hypergeometric probability distribution). 

```{r}
paste("ARI 6-profile assignment vs PAM k = 6: ", round(adjustedRandIndex(allCounts.tidy$pam.6.clustering, allCounts.tidy$group), 3))
```

Taking the ARI between the profile assignment method and the clustering method, we see it is low which makes sense given that profile assignment categorized the bulk of the genes into one of two groups (sensitive positive and sensitive negative). 


#Footnotes

1. DESeq2 from Bioconductor 

• Download the package here: https://bioconductor.org/packages/release/bioc/html/DESeq2.html 

• Read this vignette; it will be super useful!
https://www.bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html

2. When we perform multiple hypothesis tests, our type I error rate (false positives) increases. You may be familiar with the Bonferroni adjustment for multiple comparisons, which simply multiplies the p-value resulting from each test by the number of tests we performed total. However, when we perform many tests as is done in differential expression hypothesis testing, Bonferroni would give a far too large of p-values to be significant at a 0.05 confidence level. Benjamini and Hochberg developped a different, less conservative, adjustment to control the false discovery rate (FDR), or type I error rate, directly. This method takes the p-value generated from each hypothesis test and multiplies it by the total number of observational units on which the tests are performed (in our case, genes) divided by the number of observational units (genes) whose unadjusted p-value is below the confidence level (i.e.: alpha = 0.05).

# References